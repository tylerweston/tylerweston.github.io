<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c0{margin-left:36pt;padding-top:0pt;text-indent:-36pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:left}.c6{padding-top:0pt;text-indent:36pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:center;height:11pt}.c4{padding-top:0pt;text-indent:36pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:justify}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:right;height:11pt}.c2{padding-top:0pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:left;height:11pt}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c9{padding-top:0pt;text-indent:36pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:center}.c8{padding-top:0pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:justify;height:11pt}.c12{padding-top:0pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:center}.c10{-webkit-text-decoration-skip:none;color:#000000;text-decoration:line-through;vertical-align:baseline;text-decoration-skip-ink:none;font-style:normal}.c5{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c11{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c7{font-style:italic}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c11"><div><p class="c3"><span class="c13"></span></p></div><p class="c9"><span class="c1">Information Generation as a Teleological Explanation of Consciousness:</span></p><p class="c9"><span class="c1">Or, </span></p><p class="c9"><span class="c1">Of Quarks and Qualia.</span></p><p class="c6"><span class="c1"></span></p><hr style="page-break-before:always;display:none;"><p class="c6"><span class="c1"></span></p><p class="c4"><span class="c1">Block makes a distinction between access and phenomenal consciousness (henceforth A- and P-consciousness), where A-consciousness is &ldquo;[available] &nbsp;for use in reasoning and rationally guiding speech and action&rdquo; whereas P-consciousness is &ldquo;experiential properties of sensations, feelings and perceptions&rdquo; (1995). &nbsp;Functional aspects, or A-consciousness, are those that are reportable and objectively scrutinizable, via the natural sciences, whereas the subjective resides roundly outside the realm of science. The problem of fitting P-consciousness into the natural sciences is what Chalmers terms the &ldquo;Hard Problem&rdquo; (1995). How can something as subjective as experience fit into the austere, objective world of science? Perhaps this distinction is a fallacious split though; an unnatural distinction that may someday be looked back on in similar terms to Descartes&rsquo; dualism? Maybe A- and P-consciousness will turn out to be more akin to electricity and magnetism, seemingly disparate phenomena that can only fully be understood when unified and examined as different expressions of the same basic force. Furthermore, dissociative theories (those that separate A- and P-consciousness) are neither confirmable nor falsifiable, as they rely on inaccessible conscious states, so cannot form the basis of a scientific explanation (Cohen &amp; Dennett, 2011).</span></p><p class="c4"><span class="c5">So what would a natural, unified theory of consciousness look like? We can start by narrowing the space of possible explanations. We do not want a theory based on unique axioms, that is to say, no mystical or panpsychist theories, as those simply shift the explanatory burden away from consciousness and towards something more difficult to explain. Our theory should be firmly rooted in the natural sciences, and assume no new axioms. </span></p><p class="c8"><span class="c1"></span></p><p class="c4"><span class="c5">Causal structure theories of consciousness are those that depend on the components of a system interacting in the correct way. Integrated information theory (Tononi, 2004) is one such theory as it depends on the mechanisms within the system interacting in the correct manner. In IIT, feedforward networks would always have a phi of zero, while recurrent networks would always have a phi of greater than zero. This is problematic though since recurrent networks can be mathematically &ldquo;unwound&rdquo; to generate equivalent functions that are implemented in a strictly feedforward manner, since both feedforward and recurrent networks are universal function approximators, so we could get a system that, given the same initial state and input would generate the same output, yet with a phi value of zero (Doerig et al., 2019).</span><span class="c1">&nbsp;</span></p><p class="c4"><span class="c1">Sensorimotor theories of consciousness that place the entire explanatory burden on action and perception interactions or equate consciousness with perception/action also fall short as they do not explain how people who are disconnected from the environment or their bodies due to medical conditions can still be conscious, such as patients suffering locked-in syndrome (Lule et al., 2009), and likewise someone who is blind may not be conscious in the same way we are, but that is a difference of degree (if any), not type (Kupers et al., 2011). Neural correlates of consciousness theories are also not strong enough candidates for a full theory of consciousness, as they fail to address the possibility of multiple realizability of consciousness, fail to capture the important interplay of internal representation and external objects, and without strengthening of the theory merely provide correlation, not causal evidence of consciousness (Vaas, 1999). While NCCs may be necessary for consciousness to manifest in the human brain, they do not provide sufficient conditions for the general presence of consciousness.</span></p><p class="c8"><span class="c1"></span></p><p class="c4"><span class="c1">So what would an explanation of consciousness look like? Piccinini and Craver (2011) argue that a unified science of cognition can be achieved via what they term &ldquo;sketches of mechanisms&rdquo;. That is, we can give useful explanations for cognitive processes by providing functional explanations for cognitive processes that omit some of the details of the implementation. A full explanation for a phenomena then involves filling in the bottom up details of the implementation. In this paper then, I attempt to provide a &ldquo;mechanism sketch&rdquo; of consciousness, and then fill in the details both in relation to the human brain, and how this could potentially be implemented in artificial brains as well. This mechanistic explanation will therefore give us multiple realizability of consciousness, since the core of the explanation is abstracted away from the implementation (Boone &amp; Piccinini, 2016). A useful metaphor (albeit not a very original one) is software running on different computers. We can explain how and why a program functions the way it does without worrying about the nitty-gritty details of how it is executed on any specific piece of hardware, but to truly understand entirely the execution of a program, we indeed need to give an account for exactly how the sparks of high and low voltage get translated physically into the output of the program. This answers a few of the problems with other theories of consciousness: By providing a teleological, functional explanation, it avoids the pitfalls of causal theories, and relegates NCCs to a certain implementation, while not dismissing the possibility of other ways to implement </span></p><p class="c4"><span class="c1">Why do we need another theory of what consciousness is? Recent experimental work has shown much of what has traditionally been viewed as the purview of consciousness can actual happen in the absence of conscious awareness, such as attention (Koch &amp; Tsuchiya, 2007; Hsieh et al., 2011), working memory (King et al., 2016), and executive control (Lau &amp; Passingham, 2007). If none of these higher-level functions require consciousness, what exactly is the function of consciousness? Kanai et al. argue that &ldquo;generat[ing] possibly counterfactual representations using internal models learned through interactions with the environment is the function of consciousness&rdquo; (2019). This is a powerful, plausible theory, as we can suss out a reasonable evolutionary story for why we have consciousness, it can give a unified explanation of consciousness without separating A- and P-consciousness, it synergizes well with modern research in predictive processing theories of cognition, and opens up intriguing avenues while being supported by contemporary artificial intelligence research.</span></p><p class="c4"><span class="c1">One field we can use to approach a functional explanation of consciousness is evolutionary biology. By examining how and why consciousness arises, we may be able to get closer to an explanation of the function of consciousness. Dennett (1996) gives a plausible history of the evolution of consciousness, as organisms move from simple reflexive beings to the flexible and adaptive creatures we are these days. He terms these separate evolutionary creatures Darwinian, Skinnerian, Popperian, and Gregorian. The simplest creatures, the Darwinian creatures, are mere reflexive creatures, reacting to the environment through a fixed set of behaviours. The only changes in behaviour occur at a population level via random genetic mutation. This clearly does not facilitate survival of any single organism, and a change in the environment can lead to the wholesale extinction of an entire species.</span></p><p class="c4"><span class="c5">To deal with this, we get Skinnerian organisms, which can perform simple associative learning. That is, instead of acting in terms of simple input and output pairs, these organisms learn that certain actions are likely to lead to certain outcomes. That is, an individual organism may learn from instinctively performing some action that a behaviour that previously was successful in its environment is no longer leading to desirable results and so can try some other action instead. Note that the start of this stage does not necessarily entail the existence of any neural substrate, as it is possible that simple associative learning can happen within single celled organisms (Fernando et al., 2009; Ghandi et al., 2007; Hennessey et al., 1979). This is useful for when the stimuli co-occur, but if they are temporally extended we run into problems. We cannot have distinct brain structures corresponding to all possible cause-effect pairs as this would quickly lead to a combinatorial explosion, especially as we try to predict further into the future (Kanai et al., 2019). Instead of generating all possible cause-effect pairs, these organisms must begin to perform some hypothesizing or basic relevance realization, and form hypotheses between input and output. This is clearly immensely more powerful than the simple associative learning of the Skinnerian creatures. By learning that action A entails result B, by simply recalling action A, we would get a similar response to actually experiencing result B, and could base decisions on that. Simple microorganisms such as </span><span class="c5 c7">E. coli</span><span class="c5">&nbsp;and </span><span class="c5 c7">S. cerevisiae</span><span class="c1">&nbsp;can learn to adaptively anticipate changes in their environments, showing that both prokaryotic and eukaryotic organisms can learn asymmetric anticipation that allows them to &ldquo;capture the temporal connections between subsequent stimuli in their environment&rdquo; (Mitchell et al., 2009) and engage in predictive behaviour (Tagkopoulos et al., 2008). The boundary between Skinnerian and Popperian is hazy at best, as it is difficult to see how Skinnerian organisms can perform any sort of associative learning without building internal models with some sense of temporal ordering (Rescorla, 1988). </span></p><p class="c8"><span class="c1"></span></p><p class="c4"><span class="c1">According to the information generation theory of consciousness, the Popperian stage is when consciousness would arise (Kanai et al., 2019). This would be evolutionarily advantageous for organisms as it would allow them to &nbsp;realize if an action could potentially be life threatening before performing it. To move from the simpler associative learning to one that has &ldquo;temporal thickness&rdquo; requires some form of Bayesian inference. Based on Merleau-Ponty&#39;s terminology (Chouraqui, 2011), Friston (2018) suggests consciousness takes on a &ldquo;temporal thickness&rdquo; due to generative models being able to both pre- and postdict the cause and consequence of their actions. This &ldquo;temporal thickness&rdquo; is what gives us a sense of time and place, or here and nowness, as we are able to place ourselves in the large continuum of space and time via generated mental models. Kanai et al. argue that a brain is capable of processing information in real time, but to maintain models over time in a usable, adaptive manner, consciousness is necessary (2019).</span></p><p class="c4"><span class="c5">Finally, we have Gregorian creatures, who can share their mental models with other creatures. We do this through culture, where we can share our models we&rsquo;ve built through language, images, equations, or any other form of communication. This is what has allowed our species to develop and thrive, by transmitting models from person-to-person and generation-to-generation, we don&rsquo;t have to do the arduous work of generating these mental models in the first place. Other organisms besides humans may be at the plateau of Gregorian creatures, creatures as diverse as Humpback whales (Allen et al., 2013) , </span><span class="c5 c7">Bombus terrestris</span><span class="c1">, the buff-tailed bumblebee (Alem et al., 2016), and even slime moulds (Vogel &amp; Dussutour, 2016) which have all been shown to transmit relevant, adaptive behaviors between individuals. </span></p><p class="c8"><span class="c1"></span></p><p class="c4"><span class="c1">If predictive inference happens in all conscious animals, and we see myriad organisms that we would assume conscious, yet don&rsquo;t have a fully developed language (and therefore potentially access consciousness), perhaps this information generation happens at the level of phenomenal consciousness instead of access consciousness. Maybe access consciousness is epiphenomenal or emergent from the information being generated in the phenomenal consciousness, or more like an interface or veneer humans put on top of it to be able to more fully utilize consciousness. To perform useful abstraction and avoid combinatorial explosiveness, an organism must be able to filter out irrelevant information, or perform a sort of data compression, and do it all in a prelinguistic fashion. Perhaps this is what qualia is for? Consider qualia as a sort of primitive dewey decimal system, a manner in which our brain can organize the world and it&rsquo;s internal models so they can be manipulated and combined in a flexible, adaptive fashion. Back during earlier evolutionary phases, we would have had only phenomenal experiences, which would then be built upon or co-opted to become the internal representations (Humphrey, 2000), which we could then build language upon, which we would then build access consciousness upon. &nbsp;</span></p><p class="c4"><span class="c1">This explanation gives a reply to the &ldquo;Mary the neuroscientist&rdquo; thought experiment (Jackson, 1982). No matter how much Mary learns about the color red, there is always some deeper level of understanding of redness that cannot be explained linguistically, because our language is built upon these qualia. Explaining qualia via language would be like explaining quarks via apples. We may be able to learn something about them, but at the end of the day, the quarks exist at a level beneath the apple, and any apple-based explanation of quarks will necessarily be coarser grained than required to truly understand quarks. Language is used to map from higher dimensional qualia based external concepts, and internal, abstract, and accessible mental models (Neuman &amp; Nave, 2010). Different Gregorian creatures have likely developed other ways to perform this mapping, such as the whale songs or bee dances of the other Gregorian creatures mentioned earlier.</span></p><p class="c4"><span class="c1">This is all wonderful, but so far this is only the functional explanation, what does this look like in our brain? One modern theory of cognition that seeks to give a unifying framework for cognitive function is predictive processing (Rao &amp; Ballard, 1999; Friston &amp; Kiebel, 2009). This theory posits hierarchical predictions as the core of cognition. That is, generated representations at higher levels are matched up with afferent sense data and only mismatches are communicated up the layers to update models and ensure better future predictions. So, the upward flowing signal is performing data compression, since we effectively ignore wherever our downward flowing predictions match our upward flowing sense data, and the downward flow is in effect generating the sense data that we are attempting to match. This predictive mechanism facilitates the minimization of free-energy in the brain, which Friston argues is both the short-term (via the brain learning to generate accurate predictions) and long-term (via living organisms evolving so they are more fitted for their environments) as these living systems resist the slow slide into disorder and higher entropy (Friston, 2009). By generating possible future states, organisms can also estimate expected information gain from future possible actions, which can drive saliency maps, where attention is drawn to areas we expect to get the most out of observing (Kanai et al., 2019). The overall view is that the information generated top-down in a hierarchically organized fashion, where the reciprocal feedback between both lower and higher levels of prediction is what generates consciousness, via facilitating updating of internal models and allowing us to apply them to themselves in a recursive manner (Kanai et al, 2019).</span></p><p class="c4"><span class="c1">Now that we have a &ldquo;mechanism sketch&rdquo; that provides a plausible function of consciousness, we should attempt to fill in the anthropocentric implementation details. Koch et al. pick out the posterior cerebral cortex, and specifically the temporo-parietal-occipital (TPO) hot zone (2016) as a plausible candidate for the NCC. This is consistent with the global workspace (GW) theory, which points to the fronto-parietal network as being integral to making sense information available to be shared amongst multiple cortical regions (Baars, 2005). The TPO junction is heavily involved in many abstract or higher-order cognitive functions, including language, symbol processing, working memory, self-processing, and many others (De Benedictis et al., 2014). In addition, it has a large number of diffuse connections to near and distant cortical regions that allow it to perform multimodal integration of information, which is crucial to generating tightly-coupled and multimodal mental models. Highly abstract predictions involve activity in a large number of supramodal brain areas, such as middle orbital gyrus, insula, posterior medial frontal gyrus, the posterior hippocampus, and others (Weilnhammer er al., 2018). These diverse brain areas all need to be brought together somewhere so this information can be integrated and new predictions generated to be acted upon. To generate a model that involves a certain sense modality must involve activation in that area since perceiving objects require that the generative model activate those areas so that it can generate, in a top-down fashion, sensory signals that could have originated from originally encountering those objects &nbsp;(Hinton, 2007; Seth, 2014). So one possible implementation of predictive coding, and therefore consciousness, is that local, or lower level, representations are generated in their respective brain areas, where they are integrated upwards while making their way to the TPO where the highest level, abstract integration occurs. </span></p><p class="c4"><span class="c1">Based on the idea that we may be able to implement the function of consciousness in different substrates, what does that tell us about artificial consciousness and intelligence research? If single cells are capable of associative learning, what about complex silicon chips? Work on variational autoencoders (VAE) picks out and mechanizes several important features of this theory by showing how information generation from learned models can make a system an effective problem solver. VAEs consist of dual networks, one network which acts as an encoder, mapping from higher dimensional input to lower dimensional representations, and then a decoder that attempts to map these lower dimensional representations back into the original higher-dimensional data space. The network is trained using a loss function known as evidence lower bound which is similar to minimizing free energy, as explained by Friston (2009). Once these lower dimensional representations are learned, by inputting points in the latent data-space into the decoder network, we can generate new, unseen points that resemble points in our original data-space. It is not known if this is information generation of the correct type since this seems to be more &ldquo;information reorganization&rdquo; and further work still needs to be done on determining what constitutes information generation from an intrinsic perspective of a network, apart from an extrinsic observer (Kanai et al., 2019). &nbsp;</span></p><p class="c4"><span class="c1">Generative Adversarial Networks (GANs) are another model-based learning algorithm (Goodfellow et al., 2014), in which one network generates predictions and in essence tries to trick another network into believing they are real examples. By having these two networks compete with each other they synergistically boot-strap each other, with the generator becoming more adept at generating realistic seeming examples, and the discriminator getting better at distinguishing the counterfeit instances. This is interesting that these different architectures have converged on a similar solutions (generating and updating models) using different implementations, as this mirrors the &ldquo;mechanism sketches&rdquo;, where the function is the same but the implementation is radically different, similar to how we can get consciousness emerging from a variety of different substrates. This sort of hierarchical inference can be implemented in human cortical networks as approximations of Bayesian inference (Seth, 2014). &nbsp;Recent work has also shown that hardware implementations of these ideas may be fruitful as well, such as brain-like learning being implemented using synapse-like devices, that are capable of storing and recalling patterns through associative, Hebbian-like learning &nbsp;(Eryilmaz et al., 2014). According to the information generation theory, any network that performs the sort of information generation or inference stipulated by the theory would possess consciousness (Kanai et al., 2019).</span></p><p class="c4"><span class="c1">Human or not, deciphering and even defining consciousness has been and continues to be a monumental task, perhaps one of the largest and most difficult in all of the sciences. Maybe a theory of consciousness has been so difficult to find because of deeper problems in science, or a fundamental misunderstanding. Maybe an explanation of consciousness lies in the still conceptually poorly understood quantum realm (Tuszynski &amp; Woolf 2006), or an entirely more radical rethink, such as incorporating a role for information as the fundamental feature of reality (Stonier, 1996). Or maybe we are truly epistemically bound, and an answer will always elude our limited grasp. But what fun would life be if we didn&rsquo;t occasionally try the impossible? </span></p><hr style="page-break-before:always;display:none;"><p class="c2"><span class="c1"></span></p><p class="c12"><span class="c1">References:</span></p><p class="c2"><span class="c1"></span></p><p class="c0"><span class="c1">Alem, S., Perry, C. J., Zhu, X., Loukola, O. J., Ingraham, T., S&oslash;vik, E., &amp; Chittka, L. (2016). Associative Mechanisms Allow for Social Learning and Cultural Transmission of String Pulling in an Insect. PLOS Biology, 14(10), e1002564. https://doi.org/10.1371/journal.pbio.1002564</span></p><p class="c0"><span class="c1">Allen, J., Weinrich, M., Hoppitt, W., &amp; Rendell, L. (2013). Network-Based Diffusion Analysis Reveals Cultural Transmission of Lobtail Feeding in Humpback Whales. Science, 340(6131), 485&ndash;488. https://doi.org/10.1126/science.1231976</span></p><p class="c0"><span class="c1">Baars, B. J. (2005). Global workspace theory of consciousness: Toward a cognitive neuroscience of human experience. In S. Laureys (Ed.), Progress in Brain Research (Vol. 150, pp. 45&ndash;53). Elsevier. https://doi.org/10.1016/S0079-6123(05)50004-9</span></p><p class="c0"><span class="c1">Benedictis, A. D., Duffau, H., Paradiso, B., Grandi, E., Balbi, S., Granieri, E., Colarusso, E., Chioffi, F., Marras, C. E., &amp; Sarubbo, S. (2014). Anatomo-functional study of the temporo-parieto-occipital region: Dissection, tractographic and brain mapping evidence from a neurosurgical perspective. Journal of Anatomy, 225(2), 132&ndash;151. https://doi.org/10.1111/joa.12204</span></p><p class="c0"><span class="c1">Block, N. (1995). On a confusion about a function of consciousness. Behavioral and brain sciences, 18(2), 227-247.</span></p><p class="c0"><span class="c1">Boone, W., &amp; Piccinini, G. (2016). Mechanistic Abstraction. Philosophy of Science, 83(5), 686&ndash;697. https://doi.org/10.1086/687855</span></p><p class="c0"><span class="c1">Chalmers, D. J. (1995). Facing Up to the Problem of Consciousness. 27.</span></p><p class="c0"><span class="c1">Chouraqui, F. (2011). Temporal Thickness in Merleau-Ponty&rsquo;s Notes of May 1959: Chiasmi International, 13, 407&ndash;427. https://doi.org/10.5840/chiasmi20111324</span></p><p class="c0"><span class="c5">Cohen, M. A., &amp; Dennett, D. C. (2011). Consciousness cannot be separated from function. Trends in Cognitive Sciences, 15(8), 358&ndash;364. </span><span class="c5">https://doi.org/10.1016/j.tics.2011.06.008</span></p><p class="c0"><span class="c1">Dennett, D. C. (1996). The science masters series. Kinds of minds: Toward an understanding of consciousness. Basic Books.</span></p><p class="c0"><span class="c1">Doerig, A., Schurger, A., Hess, K., &amp; Herzog, M. H. (2019). The unfolding argument: Why IIT and other causal structure theories cannot explain consciousness. Consciousness and Cognition, 72, 49&ndash;59. https://doi.org/10.1016/j.concog.2019.04.002</span></p><p class="c0"><span class="c1">Eryilmaz, S. B., Kuzum, D., Jeyasingh, R., Kim, S., BrightSky, M., Lam, C., &amp; Wong, H.-S. P. (2014). Brain-like associative learning using a nanoscale non-volatile phase change synaptic device array. Frontiers in Neuroscience, 8. https://doi.org/10.3389/fnins.2014.00205</span></p><p class="c0"><span class="c1">Fernando, C. T., Liekens, A. M. L., Bingle, L. E. H., Beck, C., Lenser, T., Stekel, D. J., &amp; Rowe, J. E. (2009). Molecular circuits for associative learning in single-celled organisms. Journal of The Royal Society Interface, 6(34), 463&ndash;469. https://doi.org/10.1098/rsif.2008.0344</span></p><p class="c0"><span class="c1">Friston, K. (2009). The free-energy principle: A rough guide to the brain? Trends in Cognitive Sciences, 13(7), 293&ndash;301. https://doi.org/10.1016/j.tics.2009.04.005</span></p><p class="c0"><span class="c1">Friston, K. (2018). Am I Self-Conscious? (Or Does Self-Organization Entail Self- Consciousness?). Frontiers in Psychology, 9. https://doi.org/10.3389/fpsyg.2018.00579</span></p><p class="c0"><span class="c1">Friston, K., &amp; Kiebel, S. (2009). Predictive coding under the free-energy principle. Philosophical Transactions of the Royal Society B: Biological Sciences, 364(1521), 1211&ndash;1221. https://doi.org/10.1098/rstb.2008.0300</span></p><p class="c0"><span class="c1">Ghandi, N., Ashkenasy, G., &amp; Tannenbaum, E. (2007). Associative learning in biochemical networks. ArXiv:Q-Bio/0701010. http://arxiv.org/abs/q-bio/0701010</span></p><p class="c0"><span class="c1">Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014). Generative Adversarial Nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, &amp; K. Q. Weinberger (Eds.), Advances in Neural Information Processing Systems 27 (pp. 2672&ndash;2680). Curran Associates, Inc. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf</span></p><p class="c0"><span class="c1">Hennessey, T. M., Rucker, W. B., &amp; McDiarmid, C. G. (1979). Classical conditioning in paramecia. Animal Learning &amp; Behavior, 7(4), 417&ndash;423. https://doi.org/10.3758/BF03209695</span></p><p class="c0"><span class="c1">Hinton, G. E. (2007). To recognize shapes, first learn to generate images. In Progress in Brain Research (Vol. 165, pp. 535&ndash;547). Elsevier. https://doi.org/10.1016/S0079-6123(06)65034-6</span></p><p class="c0"><span class="c1">Hsieh, P.-J., Colas, J. T., &amp; Kanwisher, N. (2011). Pop-Out Without Awareness: Unseen Feature Singletons Capture Attention Only When Top-Down Attention Is Available. Psychological Science, 22(9), 1220&ndash;1226. https://doi.org/10.1177/0956797611419302</span></p><p class="c0"><span class="c1">Humphrey, N. (2000). How to Solve the Mind&ndash;Body Problem. 16.</span></p><p class="c0"><span class="c1">Jackson, F. (1982). Epiphenomenal qualia. The Philosophical Quarterly (1950-), 32(127), 127-136.</span></p><p class="c0"><span class="c1">Kanai, R., Chang, A., Yu, Y., Magrans de Abril, I., Biehl, M., &amp; Guttenberg, N. (2019). Information generation as a functional basis of consciousness. Neuroscience of Consciousness, 2019(1), niz016. https://doi.org/10.1093/nc/niz016</span></p><p class="c0"><span class="c1">King, J.-R., Pescetelli, N., &amp; Dehaene, S. (2016). Brain Mechanisms Underlying the Brief Maintenance of Seen and Unseen Sensory Information. Neuron, 92(5), 1122&ndash;1134. https://doi.org/10.1016/j.neuron.2016.10.051</span></p><p class="c0"><span class="c1">Kingma, D. P., &amp; Welling, M. (2014). Auto-Encoding Variational Bayes. ArXiv:1312.6114 [Cs, Stat]. http://arxiv.org/abs/1312.6114</span></p><p class="c0"><span class="c1">Koch, C., Massimini, M., Boly, M., &amp; Tononi, G. (2016). Neural correlates of consciousness: Progress and problems. Nature Reviews Neuroscience, 17(5), 307&ndash;321. https://doi.org/10.1038/nrn.2016.22</span></p><p class="c0"><span class="c1">Koch, C., &amp; Tsuchiya, N. (2007). Attention and consciousness: Two distinct brain processes. Trends in Cognitive Sciences, 11(1), 16&ndash;22. https://doi.org/10.1016/j.tics.2006.10.012</span></p><p class="c0"><span class="c5">Kupers, R., Pietrini, P., Ricciardi, E., &amp; Ptito, M. (2011). The Nature of Consciousness in the Visually Deprived Brain. Frontiers in Psychology, 2. </span><span class="c5">https://doi.org/10.3389/fpsyg.2011.00019</span></p><p class="c0"><span class="c5">Lau, H. C., &amp; Passingham, R. E. (2007). Unconscious Activation of the Cognitive Control System in the Human Prefrontal Cortex. Journal of Neuroscience, 27(21), 5805&ndash;5811. </span><span class="c5">https://doi.org/10.1523/JNEUROSCI.4335-06.2007</span></p><p class="c0"><span class="c1">Lul&eacute;, D., Zickler, C., H&auml;cker, S., Bruno, M. A., Demertzi, A., Pellas, F., ... &amp; K&uuml;bler, A. (2009). Life can be worth living in locked-in syndrome. Progress in brain research, 177, 339-351.</span></p><p class="c0"><span class="c1">Mitchell, A., Romano, G. H., Groisman, B., Yona, A., Dekel, E., Kupiec, M., Dahan, O., &amp; Pilpel, Y. (2009). Adaptive prediction of environmental changes by microorganisms. Nature, 460(7252), 220&ndash;224. https://doi.org/10.1038/nature08112</span></p><p class="c0"><span class="c1">Neuman, Y., &amp; Nave, O. (2010). Why the brain needs language in order to be self-conscious. New Ideas in Psychology, 28(1), 37&ndash;48. https://doi.org/10.1016/j.newideapsych.2009.05.001</span></p><p class="c0"><span class="c1">Piccinini, G., &amp; Craver, C. (2011). Integrating psychology and neuroscience: Functional analyses as mechanism sketches. Synthese, 183(3), 283&ndash;311. https://doi.org/10.1007/s11229-011-9898-4</span></p><p class="c0"><span class="c1">Rao, R. P. N., &amp; Ballard, D. H. (1999). Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-field effects. Nature Neuroscience, 2(1), 79&ndash;87. https://doi.org/10.1038/4580</span></p><p class="c0"><span class="c1">Rescorla, R. A. (1988). It&rsquo;s Not What You Think It Is. American Psychologist, 10.</span></p><p class="c0"><span class="c1">Seth, A. K. (2014). A predictive processing theory of sensorimotor contingencies: Explaining the puzzle of perceptual presence and its absence in synesthesia. Cognitive Neuroscience, 5(2), 97&ndash;118. https://doi.org/10.1080/17588928.2013.877880</span></p><p class="c0"><span class="c1">Stonier, T. (1996). Information as a basic property of the universe. Biosystems, 38(2), 135&ndash;140. https://doi.org/10.1016/0303-2647(96)88368-7</span></p><p class="c0"><span class="c1">Tagkopoulos, I., Liu, Y.-C., &amp; Tavazoie, S. (2008). Predictive Behavior Within Microbial Genetic Networks. 320, 6.</span></p><p class="c0"><span class="c5">Tononi, G. (2004). An information integration theory of consciousness. BMC Neuroscience, 5(1), 42. </span><span class="c1">https://doi.org/10.1186/1471-2202-5-42</span></p><p class="c0"><span class="c5">Tuszynski, J. A., &amp; Woolf, N. (2006). The path ahead. In The emerging physics of consciousness (pp. 1-26). Springer, Berlin, Heidelberg.</span></p><p class="c0"><span class="c1">Vaas, R. (1999). Why neural correlates of consciousness are fine, but not enough. Anthropology &amp; Philosophy, 3(2), 121-141.</span></p><p class="c0"><span class="c1">Vogel, D., &amp; Dussutour, A. (2016). Direct transfer of learned behaviour via cell fusion in non-neural organisms. Proceedings of the Royal Society B: Biological Sciences, 283(1845), 20162382. https://doi.org/10.1098/rspb.2016.2382</span></p><p class="c0"><span class="c5">Weilnhammer, V. A., Stuke, H., Sterzer, P., &amp; Schmack, K. (2018). The Neural Correlates of Hierarchical Predictions for Perceptual Decisions. The Journal of Neuroscience, 38(21), 5008&ndash;5021. </span><span class="c5">https://doi.org/10.1523/JNEUROSCI.2901-17.2018</span></p><p class="c2"><span class="c1"></span></p><p class="c2"><span class="c1"></span></p><p class="c2"><span class="c1"></span></p><p class="c2"><span class="c1"></span></p><p class="c2"><span class="c1"></span></p><p class="c2"><span class="c1"></span></p><p class="c2"><span class="c1"></span></p></body></html>