<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c4{padding-top:0pt;text-indent:36pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:left}.c0{padding-top:0pt;text-indent:36pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:justify}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c5{padding-top:0pt;text-indent:36pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:center}.c9{padding-top:0pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:left}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:right}.c10{padding-top:0pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:justify}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c7{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c3{height:11pt}.c2{font-style:italic}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c7"><div><p class="c3 c8"><span class="c1"></span></p><p class="c8 c3"><span class="c1"></span></p><p class="c8 c3"><span class="c1"></span></p></div><p class="c5"><span class="c1">Glial Cells &amp; Predictions:</span></p><p class="c5"><span class="c1">Inspirations from the Future of Neurobiology</span></p><p class="c5"><span class="c1">Tyler Weston</span></p><p class="c5 c3"><span class="c1"></span></p><hr style="page-break-before:always;display:none;"><p class="c4 c3"><span class="c1"></span></p><p class="c5"><span class="c1">Abstract</span></p><p class="c0"><span class="c1">Predictive coding, the idea that our cognitive processes are designed to minimize energy use by only encoding the difference in top-down prediction generated through learned representation and bottom-up information gathered from our senses, is one of the most fruitful new integrated theories of cognition. Another recent discovery is that glial cells, previously relegated to a strictly structural role in the brain, play a far more significant role in cognitive processes with their abilities to control synaptic strength, alter the architecture of the brain, and control the speed of transmission of information. This leads to a key role for glial cells in memory, learning, and many other processes that are vital to predictive coding. By looking at how glial cells offer support for the machinery of predictive coding, and current models of artificial intelligence, such as neural networks, I offer a rough framework for future research into integrating predictive coding and neuronal-glial models of cognition.</span></p><p class="c0 c3"><span class="c1"></span></p><hr style="page-break-before:always;display:none;"><p class="c0 c3"><span class="c1"></span></p><p class="c0 c3"><span class="c1"></span></p><p class="c0"><span class="c1">Imagine trying to do math with only even numbers. Or trying to write a symphony using only 6 of the 12 notes. This may seem ridiculous, but it is essentially what computer and cognitive scientists have been doing in their attempts to create artificial minds. Since the earliest days of neuroscience, most research attention has been paid to the exciting, excitable neurons, with their electrical spikes detectable by cruder, early instruments, while the nonneuronal cells, called glial cells, or simply glia, were mostly ignored, thought of as mere structural elements. After all, they appeared inert, not showing the sparks of synapses or the actions of the axons. Recently though, this view has begun to change as neuroscientists have made many important discoveries about glia and the integral role they play in the mind and human cognition, including their ability to read information from synapses, influence neuron death, and communicate with other glia using different non-electric methods (Fields, 2009). </span></p><p class="c0"><span>At the same time, another relatively new idea, predictive coding, has become one of the most promising and studied unified theories of cognition. The idea that the brain is constantly generating top-down predictions that it attempts to match with bottom-up sensory data, and only passing the difference between these two signals into higher awareness, has been used to explain many cognitive features, such as attention, perception, vision, imagery, and planning (Friston 2009; Huang and Rao, 2011). This prediction extends to lower cognitive functions as well, such as how you don&rsquo;t notice breathing until there is an error in the process, when we become acutely aware of a lack of oxygen. There are problems with current models of how this prediction would work in the human brain though, and corresponding deficiencies in artificial simulations of cognition, or artificial general intelligence (AGI). In this paper I propose why we need to incorporate a robust model of glial cells into current connectionist models in order to support the necessary machinery for developing and testing predictions.</span></p><p class="c0"><span>All cognition is prediction, from the tiny leafcutting bee brain of </span><span class="c2">M. chomskyi</span><span class="c1">&nbsp;to the decidedly larger brain of Noam Chomsky himself. With increases in cognitive functioning from mere survival to responding to and shaping the environment, not only does the total number of neurons increase, but more importantly, the ratio of glia to neurons increase as well. This increase in ratio is what allows us to move from making simple, reactive and survival based predictions, to making more useful symbolic predictions, that allow us to plan abstract and complex tasks. Birds such as the Western Scrub Jay have been shown to plan for the future, and by doing so engage in simple predictive mechanisms (Raby et al., 2007). These birds have more neurons in their forebrains by size than primates, but they have a lower ratio of glia (Olkowicz et al., 2016), thus, are not capable of making symbolic predictions (Wasserman &amp; Zentall, 2006), while primates such as Capuchin monkeys, with less neuronal density but a higher glia-neuron ratio, are capable of some degree of symbolic reasoning (Addessi et al., 2008). As we move up the animal taxonomy, we see a reliable increase in the ratio of glia to neuronal cells (Herculano-Houzel, 2014) that correspond with an increased ability to engage in symbolic predictions. The prominence of glia in the brain is a good indicator of an organism&rsquo;s ability to perform symbolic prediction, in addition to the more basic prediction necessary for survival. </span></p><p class="c0"><span class="c1">The glia-neuron ratio changes across different areas of the human brain as well. In the cerebellum, glia only account for 18.9% of cells, which makes sense as the cerebellum is primarily concerned with motor control, which is non-symbolic in nature and therefore would not require the same kind of support from glia as the rest of the brain. In the cerebral cortex, which plays a much larger role in the higher cognitive processes which require symbolic reasoning and prediction, this number jumps to 58.4% of cells being glia. This ratio jumps even higher in other areas of the brain, averaging out to 91.7% of all cells being glia, but this high number is primarily due to the lack of neuronal density in white matter, it being composed primarily of myelinated axons (Herculano-Houzel, 2014).</span></p><p class="c0"><span class="c1">Lawrence Barsalou states the predictive power of the mind rests on two central concepts: Simulators and simulations (2009). A simulator develops as we are repeatedly exposed to real world events, concepts, or objects. Being exposed to these stimuli causes activation in the relevant receptive region of the brain, which is captured and stored in astrocytes in long-term memory. As is the case with predictive coding though, we only become aware of the error as it moves up into our higher cognition, as predictions at the lower level are eliminated close to the source and not passed upward, so in this manner we don&rsquo;t remember every instance of a chair we encounter for example. If our expected chair is a hard, wooden school chair though, when we encounter a soft, leather armchair, we encode the differences between our expectations and our sensory data, ie. chairs can also be soft instead of hard, and smell of leather instead of wood. Once a simulator has been learned, we can use it to generate predictions which we store in short term memory to be manipulated or combined in various ways, and we use these simulations to plan near- and far-future actions. </span></p><p class="c0"><span>P</span><span>redictions require long-term memory to store simulators. Memory formation and storage has been linked to astrocytes, the most numerous type of glia, as they are &nbsp;able to maintain stable representations (Fields et al., 2014; Orellana et al. 2016).</span><span class="c1">&nbsp;We must be able to store and retrieve memories so we can recall and combine specific aspects of these memories to create new predictions. Studies done using functional magnetic resonance imaging have shown that recalling episodic memories activates the exact same brain area as generating future predictions (Addis et al., 2007). Interestingly though, while generating future predictions the right hippocampus becomes activated, and as this area of the brain is strongly associated with storing new memories (Komorowski et al., 2009), it could be a feedback mechanism where the brain is storing generated predictions for future retrieval. As even less intelligent animal&rsquo;s minds run on predictive mechanisms, neurons must be sufficient for generating predictions on simpler, procedural levels, but the lack of glial cells means they cannot perform higher level, symbolic predictions. </span></p><p class="c0"><span class="c1">A problem that still faces classic connectionist - that is, non-predictive, non-glia incorporating - neural networks is catastrophic interference, or catastrophic forgetting. First identified by McCloskey and Cohen (1989), this is when neural networks can abruptly and entirely forget old, learned information as new information is introduced to the system. This is unlike biological systems, where we may lose old information over time as new information is learned, but we don&rsquo;t see whole concepts or ideas wiped out as soon as we learn a new word, for example. By introducing an astrocyte based model of long-term memory though, and using the neural network as short-term memory and the primary mechanism by which information is processed, this problem could be mitigated or possibly solved. By providing a clearer separation between the fickle and fast moving neurons and synapses and the more stable astrocytes, changing synaptic weight or even the architecture of the connections between neurons should not lead to the catastrophic forgetting seen in classic neural net models.</span></p><p class="c0"><span>Not only are astrocytes necessary for long-term memory, but by allowing groups of neurons close together to communicate with each other they form small world networks that are necessary for short-term memory formation, or working memory (Roxin et al., 2004). These small world networks allow for the property of bistability, meaning they can switch between patterns of oscillation, or encoding, and stasis, or holding memories. As we use our simulators to generate symbolically driven simulations they need a way to be held in our short-term memory, and astrocytes offer an explanation for how this can work. Since individual astrocytes can take in signal from, and influence, up to 2 million neurons (Oberheim et al., 2009) and communicate between themselves through calcium signalling (Fields, 2009) they allow for formation of small world networks that can hold, exchange, and combine symbols in novel ways to create complex, symbolically driven representations in working memory.</span></p><p class="c0"><span class="c1">Prediction requires learning. We must have a mechanism for encoding new simulators into the brain. This requires not just learning new information, but new ways to process and combine the information we already have. For example, as children we learn the numbers 1 through 10, and we may soon after learn addition. This is a new way to manipulate the information we have. If we then learn how to multiply, we again learn, not new numbers, but new ways to combine the numbers we already know. This is a key distinction between the memorization of new facts, and learning of new skills for manipulating these facts. Memorizing facts increases the quantity of your knowledge, while learning increases the ways you can manipulate your knowledge. As we memorize new facts, we encode information into long-term memory in astrocytes, and as we learn new ways to manipulate data, astrocytes create new connections between different neurons or groups of neurons that lead to new ways to process information. This is consistent with the fact astrocytes are necessary for the formation, maintenance, and destruction of synaptic pathways (Ullian et al., 2004; Allen &amp; Barres, 2005; Chung et al. 2015).</span></p><p class="c10"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Also central to learning is the ability to generate and destroy neurons themselves. This is captured in the cascade-correlation model (Fahlman and Lebiere, 1990) whereby new, competing nodes are added to a neural network and only the ones that correlate more strongly with the error-rate are kept. Not only does this model allow for changing the architecture of a network, and thereby allow learning, it also intuitively captures several other ideas that point towards it being indicative of a predictive, glial based representation: it aims to minimize error, by only keeping the the nodes that match closest with the error rate in the system; by freezing the inputs to the node when the system chooses to keep a node, it is equivalent to learning a new feature predictor or simulator; and finally, it is the equivalent of the biological process that microglia, another type of glia, are responsible for (Eyo and Dailey, 2013), the destruction, or programmed cell death, of neurons no longer needed.</span></p><p class="c0"><span class="c1">Symbolic predictions require a way to coordinate areas of the brain. In subsymbolic prediction, prediction generation and error correction happens in a localized manner, for example in the optic nerve (Srinivasan et al., 1982). As we generate higher levels of prediction though, we need a mechanism to synchronize across different areas of our brain. This is achieved primarily through the work of the glia called oligodendrocytes, along with nodes of Ranvier, via modulating myelination along axonal pathways (Pajevic et al. 2014). This is necessitated by the multimodal nature of predictions, and the fact that aspects of the prediction are spread across multiple areas of the brain (Barsalou, 2003). By actively and adaptively altering the conductive velocity across axons via altering myelination, oligodendrocytes can ensure signals from different areas of the brain are coordinated and interacting in a constructive, rather than destructive manner. This is accomplished by synchronization of gamma, theta, alpha, and beta brain waves, thought to be the brain&#39;s method of remaining in sync and communicating with itself across areas (Ba&#351;ar et al., 2001). Current connectionist models do not capture this integral timing feature, and would not seem capable of coordinating areas of the network in a way capable of generating complex, multimodal predictions.</span></p><p class="c0"><span class="c1">As we move from a model of the mind that is too dissimilar to ours, towards an increasingly complex simulation of the mind, we run into problems at the far end of the spectrum as well. How brain-like does our simulation need to be? With supercomputers such as Blue Brain working on full brain emulation, while it will certainly be useful for neuroscience, what will it tell us about our cognition if we are simply emulating every cell? We may understand more about how the parts are interacting with each other but how cognition arises from a system will still be a mystery (Jolivet et al., 2015). We don&rsquo;t want to simply replicate the brain, we want to replicate the mind. We can accomplish this not by implementing glial cells at the molecular level, but rather analyzing their function and incorporating their roles into our models of the mind. We don&rsquo;t have to worry about the constituent parts of each individual cell, or, once we had the computer power, creating AGI would be a trivial affair through whole brain emulation. </span></p><p class="c0"><span class="c1">One question that arises from the study of glia: What would the exact role of microglia be in an AGI system? Microglia are necessary for the continually, life-long functioning of a healthy brain by survey the human nervous system, protecting the brain from intruders, and are an integral part to neuronal pruning that happens in developing brains, by removing dead neurons when needed (Eyo and Dailey, 2013). In an AGI system, would this require some sort of nanobot that could crawl around inside the hardware and fix physical components throughout the AGIs lifetime? Would it be a smaller weak AI that would act as garbage collectors strictly within the software realm? These could operate within the system&rsquo;s digital realm and perform software level fixes, such as maintaining code, cleaning up unused variables, clearing disconnected neurons, or fixing other software level problems. This still leaves the hardware fix level open though. Do we need to rely on a homuncular, human driven solution to perform the needed repairs to the AGI? This may occasionally be reasonable as humans sometimes require repairs from experts as well, such as in brain surgery. But for smaller day to day repairs or maintenance to the hardware running our AGI, what would this look like?</span></p><p class="c0"><span>One successful type of neural network, the convolutional neural network (CNN), seems to offer an integration of prediction and glial cells. CNNs are based on the visual cortex of the animal, which is where predictive coding was first identified and tested (Srinivasan, Laughlin, &amp; Dubs 1982; Rao &amp; Ballard, 1999). The convolutional aspect, or the smearing out, is conceptually similar to the actions of astrocytes, which communicate in clusters instead of the unidirectional nature of neurons. The fact that these two concepts blend seamlessly to create a model that, while not explicitly designed to be predi</span><span>cti</span><span class="c1">ve, offers success in real world conditions is heartening for the future. Neural network models that explicitly incorporate astrocytes have also been designed and have proven to be more efficient than traditional neural networks (Porto-Pazos et al., 2011; Alvarellos-Gonz&aacute;lez et al., 2012). While the exact role of the artificial astrocyte, and how it should be integrated into these networks, is still being studied, even across a wide range of different method for implementing astrocytes and representing a spectrum of their proposed functionality, every network that included artificial astrocytes showed improved performance over non-glial incorporating models. By further exploring and refining these neural-glial networks we may be able to further improve performance of these networks until one day reaching human level performance.</span></p><p class="c0"><span>Another type of neural network that has seen interest and success recently is generative adversarial networks (GANs) (Goodfellow et al., 2014). Here, two networks effectively compete against each other, with one network, the &ldquo;generator&rdquo;, attempting to create data, for example pictures, that can fool another &ldquo;discriminator&rdquo; network by being indistinguishable from real photographs (Sal</span><span>i</span><span class="c1">mans et al., 2016). The fact that a mathematical model that seems to capture both prediction and the effects of astrocytes in conjunction with traditional neural networks is having success in real world models of cognition offers tantalizing clues towards the success of combining glial modeling within a predictive coding network.</span></p><p class="c0"><span class="c1">Recent work done on combining these two methods into Deep Convolutional Generative Adversarial Networks has also proven successful in unsupervised learning tasks (Radford et al., 2016), one of the current goals and hard problems in AI research. These kinds of networks have been shown to be able to predict the movement of videos based on being given only the starting frames, or on video with missing frames removed. These networks do suffer the same problem classical neural networks do though, they require large amounts of data to be trained with. By incorporating a robust astrocyte network capable of holding linked representations and drawing subsets from those representations to be combined in new ways, it may be possible for these networks to learn from smaller amounts of data.</span></p><p class="c0"><span class="c1">Say, for example, we show our AGI system a video of a bouncing basketball. It may begin by recognizing the ball and using it&rsquo;s ball simulator to predict baseballs, soccer balls, volleyballs, and any other balls that it knows about will behave similarly. From there it could generalize the falling motion to its gravity simulator by recognizing the pattern that being pulled towards the earth seems to be invariant across the many instances of gravity it has encountered. This simulator is more of a function that can be applied to other concepts as opposed to the object simulator before. As above, object simulators are more something memorized while function simulators are learned. Applying this gravity function to its ball representations it has learned, it could predict the movement of a baseball bouncing, soccer ball bouncing, etc. By activating the same network that had first processed these instances, the visual center in this example, it could be able to generate new predictions that it could feed into its system, thereby taking this one instance of a bouncing ball and generating large amounts of training data it could use to increase its predictive abilities. We see this happening in human cognition when their hippocampus, used for storing new memories, becomes active during prediction &nbsp;(Komorowski et al., 2009). </span></p><p class="c0"><span class="c1">If all this work has already been done that appears conceptually similar to the incorporation of glial cells to neural networks, doesn&rsquo;t that mean this avenue is already being explored and developed? Not exactly. Most of the above examples are not directly inspired by research done towards glial cells and their roles, instead, they have arisen through distinct and differing lines of investigation, and converged on solutions that seem to point towards both predictive coding and a role for glial cells. With all of these separate lines of inquiry closing in on systems that seem to share many key concepts with the emerging theories of prediction and an increased role of glial cells in models of cognition, we may be honing in on a more complete, deeper understanding of what we would need to create an artificial general intelligence, and bring us closer to comprehending our own minds. </span></p><p class="c9 c3"><span class="c1"></span></p><hr style="page-break-before:always;display:none;"><p class="c3 c9"><span class="c1"></span></p><p class="c9 c3"><span class="c1"></span></p><p class="c4"><span>Addessi, E., Mancini, A., Crescimbene, L., Padoa-Schioppa, C., &amp; Visalberghi, E. (2008). Preference Transitivity and Symbolic Representation in Capuchin Monkeys (Cebus apella). </span><span class="c2">PLoS ONE</span><span>, </span><span class="c2">3</span><span class="c1">(6), e2414. https://doi.org/10.1371/journal.pone.0002414</span></p><p class="c4"><span>Addis, D. R., Wong, A. T., &amp; Schacter, D. L. (2007). Remembering the past and imagining the future: Common and distinct neural substrates during event construction and elaboration. </span><span class="c2">Neuropsychologia</span><span>, </span><span class="c2">45</span><span class="c1">(7), 1363&ndash;1377. https://doi.org/10.1016/j.neuropsychologia.2006.10.016</span></p><p class="c4"><span>Allen, N. J., &amp; Barres, B. A. (2005). Signaling between glia and neurons: focus on synaptic plasticity. </span><span class="c2">Current Opinion in Neurobiology</span><span>, </span><span class="c2">15</span><span class="c1">(5), 542&ndash;548. https://doi.org/10.1016/j.conb.2005.08.006</span></p><p class="c4"><span>Alvarellos-Gonz&aacute;lez, A., Pazos, A., &amp; Porto-Pazos, A. B. (2012). Computational Models of Neuron-Astrocyte Interactions Lead to Improved Efficacy in the Performance of Neural Networks. </span><span class="c2">Computational and Mathematical Methods in Medicine</span><span>, </span><span class="c2">2012</span><span class="c1">, e476324. https://doi.org/10.1155/2012/476324</span></p><p class="c4"><span>Barsalou, L. (2003). Situated simulation in the human conceptual system. </span><span class="c2">Language and Cognitive Processes</span><span>, </span><span class="c2">18</span><span class="c1">(5&ndash;6), 513&ndash;562. https://doi.org/10.1080/01690960344000026</span></p><p class="c4"><span>Barsalou, L. W. (2009). Simulation, situated conceptualization, and prediction. </span><span class="c2">Philosophical Transactions of the Royal Society B: Biological Sciences</span><span>, </span><span class="c2">364</span><span>(1521), 1281&ndash;1289. </span><span>https://doi.org/10.1098/rstb.2008.0319</span></p><p class="c4"><span>Ba&#351;ar, E., Ba&#351;ar-Eroglu, C., Karaka&#351;, S., &amp; Sch&uuml;rmann, M. (2001). Gamma, alpha, delta, and theta oscillations govern cognitive processes. </span><span class="c2">International Journal of Psychophysiology</span><span>, </span><span class="c2">39</span><span class="c1">(2), 241&ndash;248.</span></p><p class="c4"><span>Chung, W.-S., Allen, N. J., &amp; Eroglu, C. (2015). Astrocytes Control Synapse Formation, Function, and Elimination. </span><span class="c2">Cold Spring Harbor Perspectives in Biology</span><span>, </span><span class="c2">7</span><span>(9), a020370. </span><span>https://doi.org/10.1101/cshperspect.a020370</span></p><p class="c4"><span>Eyo, U. B., &amp; Dailey, M. E. (2013). Microglia: Key Elements in Neural Development, Plasticity, and Pathology. </span><span class="c2">Journal of Neuroimmune Pharmacology&#8239;: The Official Journal of the Society on NeuroImmune Pharmacology</span><span>, </span><span class="c2">8</span><span class="c1">(3), 494&ndash;509. https://doi.org/10.1007/s11481-013-9434-z</span></p><p class="c4"><span class="c1">Fahlman, S. E., &amp; Lebiere, C. (1990a). The cascade-correlation learning architecture. Retrieved from https://papers.nips.cc/paper/207-the-cascade-correlation-learning-architecture.pdf</span></p><p class="c4"><span>Fields, R. D. (2010). </span><span class="c2">The other brain: From dementia to schizophrenia, how new discoveries about the brain are revolutionizing medicine and science</span><span class="c1">. New York: Simon &amp; Schuster. </span></p><p class="c4"><span>Fields, R. D., Araque, A., Johansen-Berg, H., Lim, S.-S., Lynch, G., Nave, K.-A., &hellip; Wake, H. (2014). Glial Biology in Learning and Cognition. </span><span class="c2">The Neuroscientist</span><span>, </span><span class="c2">20</span><span class="c1">(5), 426&ndash;431. https://doi.org/10.1177/1073858413504465</span></p><p class="c4"><span>Friston, K. (2009). The free-energy principle: a rough guide to the brain? </span><span class="c2">Trends in Cognitive Sciences</span><span>, </span><span class="c2">13</span><span>(7), 293&ndash;301. </span><span>https://doi.org/10.1016/j.tics.2009.04.005</span></p><p class="c4"><span>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., &hellip; Bengio, Y. (2014). Generative adversarial nets. In </span><span class="c2">Advances in neural information processing systems</span><span class="c1">&nbsp;(pp. 2672&ndash;2680). Retrieved from http://papers.nips.cc/paper/5423-generative-adversarial-nets</span></p><p class="c4"><span>Herculano-Houzel, S. (2014). The glia/neuron ratio: How it varies uniformly across brain structures and species and what that means for brain physiology and evolution: The Glia/Neuron Ratio. </span><span class="c2">Glia</span><span>, </span><span class="c2">62</span><span class="c1">(9), 1377&ndash;1391. https://doi.org/10.1002/glia.22683</span></p><p class="c4"><span>Huang, Y., &amp; Rao, R. P. N. (2011). Predictive coding. </span><span class="c2">Wiley Interdisciplinary Reviews: Cognitive Science</span><span>, </span><span class="c2">2</span><span class="c1">(5), 580&ndash;593. https://doi.org/10.1002/wcs.142</span></p><p class="c4"><span>Jolivet, R., Coggan, J. S., Allaman, I., &amp; Magistretti, P. J. (2015). Multi-timescale modeling of activity-dependent metabolic coupling in the neuron-glia-vasculature ensemble. </span><span class="c2">PLoS Comput Biol</span><span>, </span><span class="c2">11</span><span class="c1">(2), e1004036.</span></p><p class="c4"><span>Komorowski, R. W., Manns, J. R., &amp; Eichenbaum, H. (2009). Robust Conjunctive Item-Place Coding by Hippocampal Neurons Parallels Learning What Happens Where. </span><span class="c2">Journal of Neuroscience</span><span>, </span><span class="c2">29</span><span class="c1">(31), 9918&ndash;9929. https://doi.org/10.1523/JNEUROSCI.1378-09.2009</span></p><p class="c4"><span>McCloskey, M., &amp; Cohen, N. J. (1989). Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem. In G. H. Bower (Ed.), </span><span class="c2">Psychology of Learning and Motivation</span><span class="c1">&nbsp;(Vol. 24, pp. 109&ndash;165). Academic Press. https://doi.org/10.1016/S0079-7421(08)60536-8</span></p><p class="c4"><span>Oberheim, N. A., Takano, T., Han, X., He, W., Lin, J. H., Wang, F., &hellip; Nedergaard, M. (2009). Uniquely hominid features of adult human astrocytes. </span><span class="c2">The Journal of Neuroscience&#8239;: The Official Journal of the Society for Neuroscience</span><span>, </span><span class="c2">29</span><span class="c1">(10), 3276. https://doi.org/10.1523/JNEUROSCI.4707-08.2009</span></p><p class="c4"><span>Olkowicz, S., Kocourek, M., Lu&#269;an, R. K., Porte&scaron;, M., Fitch, W. T., Herculano-Houzel, S., &amp; N&#283;mec, P. (2016). Birds have primate-like numbers of neurons in the forebrain. </span><span class="c2">Proceedings of the National Academy of Sciences</span><span>, </span><span class="c2">113</span><span class="c1">(26), 7255&ndash;7260. https://doi.org/10.1073/pnas.1517131113</span></p><p class="c4"><span>Orellana, J. A., Retamal, M. A., Moraga-Amaro, R., &amp; Stehberg, J. (2016). Role of Astroglial Hemichannels and Pannexons in Memory and Neurodegenerative Diseases. </span><span class="c2">Frontiers in Integrative Neuroscience</span><span>, </span><span class="c2">10</span><span class="c1">. https://doi.org/10.3389/fnint.2016.00026</span></p><p class="c4"><span>Pajevic, S., Basser, P. J., &amp; Fields, R. D. (2014). Role of Myelin Plasticity in Oscillations and Synchrony of Neuronal Activity. </span><span class="c2">Neuroscience</span><span>, </span><span class="c2">276</span><span>, 135&ndash;147. </span><span class="c1">https://doi.org/10.1016/j.neuroscience.2013.11.007</span></p><p class="c4"><span>Porto-Pazos, A. B., Veiguela, N., Mesejo, P., Navarrete, M., Alvarellos, A., Ib&aacute;&ntilde;ez, O., &hellip; Araque, A. (2011). Artificial Astrocytes Improve Neural Network Performance. </span><span class="c2">PLoS ONE</span><span>, </span><span class="c2">6</span><span>(4), e19109. https://doi.org/10.1371/journal.pone.0019109</span></p><p class="c4"><span>Raby, C. R., Alexis, D. M., Dickinson, A., &amp; Clayton, N. S. (2007). Planning for the future by western scrub-jays. </span><span class="c2">Nature</span><span>, </span><span class="c2">445</span><span class="c1">(7130), 919&ndash;921. https://doi.org/10.1038/nature05575</span></p><p class="c4"><span>Radford, A., Metz, L., &amp; Chintala, S. (2015). Unsupervised representation learning with deep convolutional generative adversarial networks. </span><span class="c2">arXiv Preprint arXiv:1511.06434</span><span class="c1">. Retrieved from https://arxiv.org/abs/1511.06434</span></p><p class="c4"><span>Roxin, A., Riecke, H., &amp; Solla, S. A. (2004). Self-Sustained Activity in a Small-World Network of Excitable Neurons. </span><span class="c2">Physical Review Letters</span><span>, </span><span class="c2">92</span><span class="c1">(19). https://doi.org/10.1103/PhysRevLett.92.198101</span></p><p class="c4"><span>Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., &amp; Chen, X. (2016). Improved Techniques for Training GANs. </span><span class="c2">arXiv:1606.03498 [Cs]</span><span class="c1">. Retrieved from http://arxiv.org/abs/1606.03498</span></p><p class="c4"><span>Srinivasan, M. V., Laughlin, S. B., &amp; Dubs, A. (1982). Predictive coding: a fresh view of inhibition in the retina. </span><span class="c2">Proceedings of the Royal Society of London B: Biological Sciences</span><span>, </span><span class="c2">216</span><span class="c1">(1205), 427-459.</span></p><p class="c4"><span>Ullian, E. M., Christopherson, K. S., &amp; Barres, B. A. (2004). Role for glia in synaptogenesis. </span><span class="c2">Glia</span><span>, </span><span class="c2">47</span><span class="c1">(3), 209&ndash;216. https://doi.org/10.1002/glia.20082</span></p><p class="c4"><span>Wasserman, E. A., &amp; Zentall, T. R. (2006). </span><span class="c2">Comparative Cognition: Experimental Explorations of Animal Intelligence</span><span class="c1">. Oxford University Press.</span></p><p class="c4"><span>Zhao, M., Zhuang, C., Wang, Y., &amp; Lee, T. S. (2014). Predictive encoding of contextual relationships for perceptual inference, interpolation and prediction. </span><span class="c2">arXiv Preprint arXiv:1411.3815</span><span class="c1">. Retrieved from https://arxiv.org/abs/1411.3815</span></p><p class="c4 c3"><span class="c1"></span></p><p class="c4 c3"><span class="c1"></span></p><p class="c4 c3"><span class="c1"></span></p><p class="c9 c3"><span class="c1"></span></p><div><p class="c3 c6"><span class="c1"></span></p></div></body></html>